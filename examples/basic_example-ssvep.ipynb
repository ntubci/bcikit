{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook\n",
    "\n",
    "This notebook showcase how we can load an EEG dataset, and perform preprocessing. We train the EEGNet model with leave one subject paradigm and k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\github\\bcikit\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load subject IDs [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "from bcikit.dataclass import BcikitConfig\n",
    "\n",
    "\n",
    "load_subject_id_range = (1, 3)\n",
    "subject_ids = list(np.arange(load_subject_id_range[0], load_subject_id_range[1]+1, dtype=int))\n",
    "print(\"Load subject IDs\", subject_ids)\n",
    "\n",
    "config = {\n",
    "    \"dataset\": {\n",
    "        \"root\": \"../data/hsssvep\",\n",
    "        \"num_channels\": 9, # although this dataset has 64 channels, we planned to perform preprocessing, which left us with 9 channels\n",
    "        \"num_classes\": 40, # this is not used in the dataloading process, but it belongs to the dataset, so lets define it here\n",
    "        \"subject_ids\": subject_ids, # this is a list of int\n",
    "        \"sessions\": None, # `None` since this dataset has one session\n",
    "        \"do_recommended_preprocessing\": True # perform preprocessing defined in the dataset\n",
    "    },\n",
    "    \"optimizer\": { # work in progress, we need this when we build Trainer\n",
    "        \"weight_decay\": 0.05,\n",
    "    },\n",
    "    \"lr_scheduler\": { # work in progress, we need this when we build Trainer\n",
    "        \"learning_rate\": 0.001,\n",
    "    },\n",
    "    \"criterion\": { # work in progress, we need this when we build Trainer\n",
    "        \n",
    "    },\n",
    "    \"model\": { # work in progress, we need this when we build Trainer\n",
    "        \n",
    "    },\n",
    "    \"training\": { # work in progress, we need this when we build Trainer\n",
    "        \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        \"batchsize\": 64,\n",
    "        \"kfold_split\": 3,\n",
    "        \"num_epochs\": 10,\n",
    "    },\n",
    "    \"log_level\": 1 # 0: no logs | 1: minimum, only the very important ones | 2: moderate, tells you at which steps is the code running | 3: everything, for serious debugging\n",
    "}\n",
    "\n",
    "config = BcikitConfig(config) # we do this, so we can use dot notation. this is work in progress, to design a global config, see https://github.com/ntubci/bcikit/issues/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create preprocessing function\n",
    "\n",
    "We have a custom preprocessing function. Here we slice the 4-second signal into 4 parts, and we select only the first segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bcikit.transforms.segment_time import segment_data_time_domain\n",
    "\n",
    "\n",
    "def preprocessing(data, targets, channel_names, sample_rate, segment_config, verbose, **kwargs):\n",
    "    print()\n",
    "    print(\"preprocessing data shape\", data.shape) # (subject,session,trial,channel,time)\n",
    "\n",
    "    # segment signal and select the first segment\n",
    "    data = segment_data_time_domain(\n",
    "        data=data,\n",
    "        window_len=segment_config.window_len,\n",
    "        shift_len=segment_config.shift_len,\n",
    "        sample_rate=segment_config.sample_rate,\n",
    "        add_segment_axis=True,\n",
    "    )\n",
    "    data = data[:, :, :, :, 0, :] # select the first segment, and remove the rest\n",
    "\n",
    "    # since we are doing leave one subject out, we don't care about `session`, we only want data in this format (subject, trial, channel, time).\n",
    "    data = data.reshape((data.shape[0], data.shape[1]*data.shape[2], data.shape[3], data.shape[4]))\n",
    "    targets = targets.reshape((targets.shape[0], targets.shape[1]*targets.shape[2]))\n",
    "    \n",
    "    print(\"final data.shape\", data.shape)\n",
    "    print(\"final targets.shape\", targets.shape)\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We load and preprocess the dataset that want to use in our experiment. We only do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load subject: 1\n",
      "Load subject: 2\n",
      "Load subject: 3\n",
      "\n",
      "preprocessing data shape (3, 1, 240, 9, 1000)\n",
      "final data.shape (3, 240, 9, 250)\n",
      "final targets.shape (3, 240)\n"
     ]
    }
   ],
   "source": [
    "from bcikit.datasets.ssvep import HSSSVEP\n",
    "from bcikit.datasets import EEGDataloader\n",
    "from bcikit.datasets.data_selection_methods import leave_one_subject_out\n",
    "\n",
    "\n",
    "segment_config = {\n",
    "    \"window_len\": 1,\n",
    "    \"shift_len\": 250,\n",
    "    \"sample_rate\": 250,\n",
    "}\n",
    "segment_config = BcikitConfig(segment_config) # we do this, so we can use dot notation, `segment_config.window_len` is nicer then doing `segment_config[\"window_len\"]`\n",
    "\n",
    "data = EEGDataloader(\n",
    "    dataset=HSSSVEP, \n",
    "    config=config,\n",
    "    preprocessing_fn=preprocessing, # customize your own preprocessing, pass `None` if do not have customized preprocessing steps\n",
    "    data_selection_fn=leave_one_subject_out, # customize your data selection function or use common ones from `data_selection_methods`\n",
    "    # start kwargs: basically you can pass any additional params you have\n",
    "    segment_config=segment_config, # in this case, `segment_config` is needed in the custom `preprocessing` function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We import CompactEEGNet and make sure it is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 9, 250])\n",
      "Output shape: torch.Size([16, 40])\n",
      "Model size: 63304\n"
     ]
    }
   ],
   "source": [
    "from bcikit.models import CompactEEGNet\n",
    "from bcikit.models.utils import count_params\n",
    "\n",
    "\n",
    "model = CompactEEGNet(\n",
    "    num_channel=config.dataset.num_channels,\n",
    "    num_classes=config.dataset.num_classes,\n",
    "    signal_length=segment_config.window_len * segment_config.sample_rate,\n",
    ").to(config.training.device)\n",
    "\n",
    "x = torch.ones((16, config.dataset.num_channels, segment_config.window_len * segment_config.sample_rate)).to(config.training.device)\n",
    "y = model(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", y.shape)\n",
    "print('Model size:', count_params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Here shows how we can use the dataloader, and train a model.\n",
    "\n",
    "We call the `data.get_dataloaders` here because we want to fetch different k-fold and different \"leave one subject out\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loader: (320, 9, 250) (320,)\n",
      "val_loader: (160, 9, 250) (160,)\n",
      "test_loader: (240, 9, 250) (240,)\n"
     ]
    }
   ],
   "source": [
    "test_subject_id = 1\n",
    "kfold_k = 0\n",
    "\n",
    "train_loader, val_loader, test_loader = data.get_dataloaders(test_subject_id=test_subject_id, batchsize=config.training.batchsize, kfold_k=kfold_k, kfold_split=config.training.kfold_split)\n",
    "\n",
    "print()\n",
    "print(\"train_loader:\", train_loader.dataset.data.shape, train_loader.dataset.targets.shape)\n",
    "print(\"val_loader:\", val_loader.dataset.data.shape, val_loader.dataset.targets.shape)\n",
    "print(\"test_loader:\", test_loader.dataset.data.shape, test_loader.dataset.targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "optimizer = optim.Adam(params_to_update, lr=config.lr_scheduler.learning_rate, weight_decay=config.optimizer.weight_decay)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: 18.70075\n",
      "Epoch 2 - loss: 18.20019\n",
      "Epoch 3 - loss: 17.73731\n",
      "Epoch 4 - loss: 17.29030\n",
      "Epoch 5 - loss: 16.97092\n",
      "Epoch 6 - loss: 16.61557\n",
      "Epoch 7 - loss: 15.97013\n",
      "Epoch 8 - loss: 15.27393\n",
      "Epoch 9 - loss: 14.45492\n",
      "Epoch 10 - loss: 13.29764\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader, device):\n",
    "    for epoch in range(config.training.num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for X, Y in train_loader:\n",
    "            inputs = X.to(device)\n",
    "            labels = Y.long().to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch {} - loss: {:.5f}'.format(epoch+1, epoch_loss))\n",
    "        \n",
    "train(train_loader, config.training.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
